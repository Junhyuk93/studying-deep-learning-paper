# VIT

## RNN to Transformer in NLP

- RNNì€ ì˜¤ëœì‹œê°„ë™ì•ˆ NLP ë¶„ì•¼ì—ì„œ ê¸°ë³¸ ëª¨ë¸ë¡œì¨ í™œìš©ë¨
- Transformerì˜ ë“±ì¥ ì´í›„ Transformer ì¤‘ì‹¬ìœ¼ë¡œ ì—°êµ¬ê°€ ì§„í–‰ë¨

![](https://i.imgur.com/RltkRPP.png)

## Transformer in Computer Vision

- **ì˜ˆì „ì—ëŠ” CNNì— self attentionì„ ì ìš©ì‹œí‚¤ëŠ” ë°©ë²•ì„ ê³ ë¯¼í•¨ğŸ¤”**
    
    - Non-local neural networks
    ![](https://i.imgur.com/tRu4EQo.png)

    - Stand-alone self-attention in vision models
    ![](https://i.imgur.com/UJbUvls.png)

    - Axial-DeepLab
    ![](https://i.imgur.com/cPdqlcj.png)


- **ì‚¬ê³ ë°©ì‹ì˜ ì „í™˜ìœ¼ë¡œ Transformer ëª¨ë¸ ìì²´ë¥¼ ì´ìš©í•´ Vision taskë¥¼ í’€ì–´ë³´ì!ğŸ˜®**

    - Vision Transformer
    - Data efficient image Transformer
    - TransGAN (??)

## Tansformer and Self Attention
- Transformer : **Attention** ë§Œì„ í™œìš©í•´ ëª¨ë¸ êµ¬ì¶•
- Transformerì˜ í•µì‹¬ ì•„ì´ë””ì–´ 

## Seq2seq
- Seq2seq : ë¬¸ì¥ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë¬¸ì¥ì„ ì¶œë ¥í•˜ëŠ” ëª¨ë¸ / ê¸°ê³„ë²ˆì—­ì— ì£¼ë¡œ ì‚¬ìš©
- Context vector: Decoderì—ê²Œ ì „ë‹¬ë˜ëŠ” ì…ë ¥ ë¬¸ì¥ì˜ ì •ë³´
- Context vectorì˜ í¬ê¸°ê°€ ì œí•œì ì´ê¸° ë•Œë¬¸ì— ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ì •ë³´ë¥¼ ì „í•˜ê¸° ì–´ë ¤ì›€!!

![](https://i.imgur.com/JpPqA6H.png)

## Seq2seq with Attention
- Decoderê°€ íŠ¹ì • ì‹œì  ë‹¨ì–´ë¥¼ ì¶œë ¥í•  ë•Œ encoder ì •ë³´ ì¤‘ ì—°ê´€ì„±ì´ ìˆëŠ” ì •ë³´ë¥¼ ì§ì ‘ ì„ íƒ!!


![](https://i.imgur.com/rYU7LWw.png)


decoderì˜ ì²«ë²ˆì§¸ ì‹œì ê³¼ encdoer ê° ì‹œì ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì´ê²ƒì„ weightë¡œ í™œìš©í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ encoder ê° ì‹œì ì˜ ì •ë³´ë“¤ì„ í†µí•©í•˜ì—¬ Context Vectorë¥¼ ë§Œë“¬! 

Context Vectorë¥¼ decoderì˜ ì²«ë²ˆì§¸ ì‹œì  ë‹¨ì–´ë¥¼ ì¶œë ¥í• ë•Œ í™œìš©í•˜ì—¬ ì¢€ ë” ì •í™•í•œ ì¶œë ¥ê°’ì„ ë‚´ë±‰ì„ ìˆ˜ ìˆê²Œ ë¨!

![](https://i.imgur.com/FxFaOJi.png)

## Attention vs Self Attention
- Attention (Decoder  Query / Encoder â†’ Key, Value) / encoder, decoder ì‚¬ì´ì˜ ìƒê´€ê´€ê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
- Self attention (ì…ë ¥ ë°ì´í„° â†’ Query, Key, Value) / ë°ì´í„° ë‚´ì˜ ìƒê´€ê´€ê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ

![](https://i.imgur.com/nn15dsp.png)


## Transformer vs CNN

- CNN : ì´ë¯¸ì§€ ì „ì²´ì˜ ì •ë³´ë¥¼ í†µí•©í•˜ê¸° ìœ„í•´ì„œëŠ” ëª‡ ê°œì˜ layer í†µê³¼
- Transformer : í•˜ë‚˜ì˜ alyerë¡œ ì „ì²´ ì´ë¯¸ì§€ ì •ë³´ í†µí•© ê°€ëŠ¥

![](https://i.imgur.com/EPOuhxx.png)

## Inductive bias

- Inductive bias : ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ ëª¨ë¸ì— ì‚¬ì „ì ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ê°€ì •
- SVM : Margin ìµœëŒ€í™” / CNN : ì§€ì—­ì ì¸ ì •ë³´ / RNN : ìˆœì°¨ì ì¸ ì •ë³´

![](https://i.imgur.com/Z30H2eH.png)

- Transformer
    1ì°¨ì› ë²¡í„°ë¡œ ë§Œë“  í›„ self attention (2ì°¨ì›ì˜ ì§€ì—­ì ì¸ ì •ë³´ ìœ ì§€ X)
    Weightì´ inputì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ ë³€í•¨
    
- CNN
    2ì°¨ì›ì˜ ì§€ì—­ì ì¸ íŠ¹ì„± ìœ ì§€
    í•™ìŠµ í›„ weight ê³ ì •ë¨!
    
<center>Transformer : inductive bias â†“ , ëª¨ë¸ì˜ ììœ ë„ â†‘</center>

![](https://i.imgur.com/6kIdyZT.png)

- **ViTì˜ ë‹¨ì  : ì´ë¯¸ì§€ì˜ 2ì°¨ì›ì  ì •ë³´ë¥¼ ìœ ì§€í•˜ì§€ ëª»í•´ inductive biasë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì–´ë µê³  ì´ì— ë”°ë¼ ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•¨!!**

## VIT

### Abstract

- CNNì„ ì‚¬ìš©í•  í•„ìš” ì—†ì´ imageë¥¼ sequence of patchesë¡œ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” transformer ëª¨ë¸ ìì²´ê°€  classificationì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒ! ğŸ˜®
- SOTAì˜ CNN ê¸°ë°˜ ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥

![](https://i.imgur.com/GhfNwIA.png)


### Introduction

- imageë¥¼ ì˜ë¼ì„œ patch(treated the same way as tokens (words))ë¡œ ë§Œë“¤ê³  sequenceë¥¼ linear embeddingìœ¼ë¡œ ë§Œë“¤ì–´ transfoerì— ë„£ì—ˆìŒ!!

![](https://i.imgur.com/hyFW18t.png)

input image ê°€ ì „ì²´ ë¬¸ì¥ì´ê³ , image patchê°€ ë¬¸ì¥ì„ ì´ë£¨ëŠ” ê°ê°ì˜ ë‹¨ì–´ë¼ê³  ì´í•´í•˜ë©´ í¸í•¨!


![](https://i.imgur.com/jVg8t7G.png)


- â‘  Classification token : classificationì„ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” token (BERT [CLS] token)
- â‘¡ Position embedding : patchì˜ ìœ„ì¹˜ ì •ë³´
- â‘ ,â‘¡ ëŠ” í•™ìŠµì„ í†µí•´ ê²°ì •ë¨!
- (Classification token, Patch embedding) + Positional embedding = Transformer encoder ì…ë ¥



![](https://i.imgur.com/oipI8bF.png)

- "Vanilla" Transformer encoder vs "ViT" Transformer
- Layer normealizationì˜ ìœ„ì¹˜ê°€ Transformer í•™ìŠµì— ì¤‘ìš”í•œ ì—­í• ì„ ë¼ì¹¨(Learning Deep Trasformer Models for Machine Translation)
- ë”°ë¼ì„œ ViT ëŠ” ìˆ˜ì •ëœ Transformer encoderë¥¼ ì ìš©í•¨! (Normalizationì„ ë¨¼ì € ì ìš©)

### Transformer encdoer : Self attention

![](https://i.imgur.com/FSyBC7B.png)

- Layer Normalizationì€ instance ë‹¨ìœ„ë¡œ nomalization í•œë‹¤ê³  ìƒê°í•˜ë©´ ë¨!

![](https://i.imgur.com/Y4XQkKp.png)

- Transformer encoder: Self attention
- Encoderì˜ ì…ë ¥(z) â†’ query, key, value ë²¡í„° / W matrix: í•™ìŠµë˜ëŠ” íŒŒë¼ë¯¸í„°

ì‹¤ì œë¡œ í•™ìŠµì´ ì¼ì–´ë‚˜ëŠ”ê±´ W matrixì„ í†µí•´ attentionì´ í›ˆë ¨ë¨.

![](https://i.imgur.com/qqgHcN7.png)

- query, key, value ì— ë™ì¼í•œ W matrixê°€ ê³±í•´ì§€ê³  Q,K,Vê°€ ê³„ì‚°ë¨!

- ê°ê°ì˜ query, keyê°€ dot productë¥¼ í†µí•´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³  softmax í•¨ìˆ˜ë¥¼ í†µí•´ 0~1 ì‚¬ì´ì˜ ê°’ì˜ attention scoreë¥¼ ê³„ì‚°í•¨!

- valueê°’ê³¼ attention scoreë¥¼ ê³±í•´ì£¼ê³  ë”í•´ì¤Œìœ¼ë¡œì¨ ìµœì¢… attentionì˜ outputì„ ê³„ì‚°í•¨!


### Transformer encoder : Multi-Head Self attention

- ViTì—ì„œëŠ” Self attention 12ë²ˆ ìˆ˜í–‰

![](https://i.imgur.com/QocnowL.png)


### Transformer encoder : MLP

![](https://i.imgur.com/sDT09Ay.png)

- MLPì—ì„œëŠ” 2ë‹¨ìœ¼ë¡œ í™œì„±í™” í•¨ìˆ˜ì¸ GELUë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŒ! 
- GELUê°€ ë­ì•¼?ğŸ¤¨ â†’ [GELU](https://arxiv.org/abs/1606.08415)

### Transformer output

![](https://i.imgur.com/SiqDtSp.png)